{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_EhxGKMF44VT",
        "ivdQ9FD741fX",
        "9z9gKibg7FJa",
        "w_UopAK34y6E",
        "tiNVxW5DZbHP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PA2"
      ],
      "metadata": {
        "id": "jqtpUpn7Cwv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "@author: jcheung\n",
        "\n",
        "Developed for Python 2. Automatically converted to Python 3; may result in bugs.\n",
        "'''\n",
        "import xml.etree.cElementTree as ET\n",
        "import codecs\n",
        "\n",
        "class WSDInstance:\n",
        "    def __init__(self, my_id, lemma, context, index):\n",
        "        self.id = my_id         # id of the WSD instance\n",
        "        self.lemma = lemma      # lemma of the word whose sense is to be resolved\n",
        "        self.context = context  # lemma of all the words in the sentential context\n",
        "        self.index = index      # index of lemma within the context\n",
        "    def __str__(self):\n",
        "        '''\n",
        "        For printing purposes.\n",
        "        '''\n",
        "        return '%s\\t%s\\t%s\\t%d' % (self.id, self.lemma, ' '.join(self.context), self.index)\n",
        "\n",
        "def load_instances(f):\n",
        "    '''\n",
        "    Load two lists of cases to perform WSD on. The structure that is returned is a dict, where\n",
        "    the keys are the ids, and the values are instances of WSDInstance.\n",
        "    '''\n",
        "    tree = ET.parse(f)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    dev_instances = {}\n",
        "    test_instances = {}\n",
        "\n",
        "    for text in root:\n",
        "        if text.attrib['id'].startswith('d001'):\n",
        "            instances = dev_instances\n",
        "        else:\n",
        "            instances = test_instances\n",
        "        for sentence in text:\n",
        "            # construct sentence context\n",
        "            context = [to_ascii(el.attrib['lemma']) for el in sentence]\n",
        "            for i, el in enumerate(sentence):\n",
        "                if el.tag == 'instance':\n",
        "                    my_id = el.attrib['id']\n",
        "                    lemma = to_ascii(el.attrib['lemma'])\n",
        "                    instances[my_id] = WSDInstance(my_id, lemma, context, i)\n",
        "    return dev_instances, test_instances\n",
        "\n",
        "def load_key(f):\n",
        "    '''\n",
        "    Load the solutions as dicts.\n",
        "    Key is the id\n",
        "    Value is the list of correct sense keys.\n",
        "    '''\n",
        "    dev_key = {}\n",
        "    test_key = {}\n",
        "    for line in open(f):\n",
        "        if len(line) <= 1: continue\n",
        "        #print (line)\n",
        "        doc, my_id, sense_key = line.strip().split(' ', 2)\n",
        "        if doc == 'd001':\n",
        "            dev_key[my_id] = sense_key.split()\n",
        "        else:\n",
        "            test_key[my_id] = sense_key.split()\n",
        "    return dev_key, test_key\n",
        "\n",
        "def to_ascii(s):\n",
        "    # Remove all non-ASCII characters and ensure the result is a Unicode string\n",
        "    return codecs.encode(s, 'ascii', 'ignore').decode('ascii')\n"
      ],
      "metadata": {
        "id": "4GBXpJ5vC6GJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    data_f = 'multilingual-all-words.en.xml'\n",
        "    key_f = 'wordnet.en.key'\n",
        "    dev_instances, test_instances = load_instances(data_f)\n",
        "    dev_key, test_key = load_key(key_f)\n",
        "\n",
        "    # IMPORTANT: keys contain fewer entries than the instances; need to remove them\n",
        "    dev_instances = {k:v for (k,v) in dev_instances.items() if k in dev_key}\n",
        "    test_instances = {k:v for (k,v) in test_instances.items() if k in test_key}\n",
        "\n",
        "    # read to use here\n",
        "    print(len(dev_instances)) # number of dev instances\n",
        "    print(len(test_instances)) # number of test instances\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2kqHaYWGeyp",
        "outputId": "198ac273-537d-4219-fa4a-af1b2833f500"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "194\n",
            "1450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary NLTK and other libraries\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.wsd import lesk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Initialize stop words and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaF1KqrlGoTP",
        "outputId": "8d94b828-505e-4d1c-a180-90381e872302"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(predictions, gold):\n",
        "    '''\n",
        "    Evaluate the predictions against the gold standard.\n",
        "    '''\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for inst_id, sense_keys in gold.items():\n",
        "        total += 1\n",
        "        predicted_key = predictions.get(inst_id)\n",
        "        if predicted_key is None:\n",
        "            continue  # No prediction\n",
        "        if predicted_key in sense_keys:\n",
        "            correct += 1\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "B3fIwhNdFtpJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Most frequent sense and LESK"
      ],
      "metadata": {
        "id": "_EhxGKMF44VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    '''\n",
        "    Preprocess text by tokenizing, lowercasing, removing stop words, and lemmatizing.\n",
        "    '''\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [t.lower() for t in tokens if t.isalpha()]\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return set(tokens)\n",
        "\n",
        "def most_frequent_sense(instances):\n",
        "    '''\n",
        "    Implement the most frequent sense baseline.\n",
        "    '''\n",
        "    predictions = {}\n",
        "    for inst_id, instance in instances.items():\n",
        "        lemma = instance.lemma\n",
        "        # Get synsets for the lemma\n",
        "        synsets = wn.synsets(lemma)\n",
        "        if synsets:\n",
        "            synset = synsets[0]  # Most frequent sense\n",
        "            # Get sense key corresponding to the lemma\n",
        "            sense_keys = [l.key() for l in synset.lemmas() if l.name() == lemma]\n",
        "            if sense_keys:\n",
        "                predictions[inst_id] = sense_keys[0]\n",
        "            else:\n",
        "                predictions[inst_id] = synset.lemmas()[0].key()\n",
        "        else:\n",
        "            predictions[inst_id] = None  # No prediction\n",
        "    return predictions\n",
        "\n",
        "def lesk_algorithm(instances):\n",
        "    '''\n",
        "    Implement NLTK's Lesk algorithm.\n",
        "    '''\n",
        "    predictions = {}\n",
        "    for inst_id, instance in instances.items():\n",
        "        context_sentence = ' '.join(instance.context)\n",
        "        target_word = instance.context[instance.index]\n",
        "        # Apply Lesk algorithm\n",
        "        synset = lesk(instance.context, target_word)\n",
        "        if synset:\n",
        "            sense_keys = [l.key() for l in synset.lemmas() if l.name() == target_word]\n",
        "            if sense_keys:\n",
        "                predictions[inst_id] = sense_keys[0]\n",
        "            else:\n",
        "                predictions[inst_id] = synset.lemmas()[0].key()\n",
        "        else:\n",
        "            predictions[inst_id] = None\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "OICrT7XtCyVX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Most Frequent Sense Baseline on Dev Set\n",
        "dev_mfs_predictions = most_frequent_sense(dev_instances)\n",
        "dev_mfs_accuracy = evaluate(dev_mfs_predictions, dev_key)\n",
        "print('Dev Most Frequent Sense Baseline Accuracy:', dev_mfs_accuracy)\n",
        "\n",
        "# Evaluate Lesk Algorithm on Dev Set\n",
        "dev_lesk_predictions = lesk_algorithm(dev_instances)\n",
        "dev_lesk_accuracy = evaluate(dev_lesk_predictions, dev_key)\n",
        "print('Dev Lesk Algorithm Accuracy:', dev_lesk_accuracy)\n",
        "\n",
        "# Similarly evaluate on Test Set\n",
        "test_mfs_predictions = most_frequent_sense(test_instances)\n",
        "test_mfs_accuracy = evaluate(test_mfs_predictions, test_key)\n",
        "print('Test Most Frequent Sense Baseline Accuracy:', test_mfs_accuracy)\n",
        "\n",
        "test_lesk_predictions = lesk_algorithm(test_instances)\n",
        "test_lesk_accuracy = evaluate(test_lesk_predictions, test_key)\n",
        "print('Test Lesk Algorithm Accuracy:', test_lesk_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhaCzl-UEu51",
        "outputId": "dfc77b10-1fa9-4767-b6de-e57d1a53cd58"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Most Frequent Sense Baseline Accuracy: 0.6752577319587629\n",
            "Dev Lesk Algorithm Accuracy: 0.32989690721649484\n",
            "Test Most Frequent Sense Baseline Accuracy: 0.6172413793103448\n",
            "Test Lesk Algorithm Accuracy: 0.33655172413793105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAMPLE OUTPUT**"
      ],
      "metadata": {
        "id": "5EPyCLhcmUCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the first instance from dev_instances\n",
        "first_instance_id = next(iter(dev_instances))\n",
        "first_instance = {first_instance_id: dev_instances[first_instance_id]}\n",
        "print(first_instance.items())\n",
        "\n",
        "# Test the most_frequent_sense function\n",
        "single_mfs_prediction = most_frequent_sense(first_instance)\n",
        "print(\"Single Instance MFS Prediction:\", single_mfs_prediction)\n",
        "\n",
        "# Test the lesk_algorithm function\n",
        "single_lesk_prediction = lesk_algorithm(first_instance)\n",
        "print(\"Single Instance Lesk Prediction:\", single_lesk_prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOLAX2jWmSxV",
        "outputId": "0c82d9bd-c660-48ad-a105-f22f950995d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('d001.s001.t002', <__main__.WSDInstance object at 0x7dd58196edd0>)])\n",
            "Single Instance MFS Prediction: {'d001.s001.t002': 'group%1:03:00::'}\n",
            "Single Instance Lesk Prediction: {'d001.s001.t002': 'group%2:33:00::'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BERT"
      ],
      "metadata": {
        "id": "ivdQ9FD741fX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def bert_wsd(instances):\n",
        "    '''\n",
        "    Implement WSD using a pretrained BERT model.\n",
        "    '''\n",
        "    predictions = {}\n",
        "    for inst_id, instance in instances.items():\n",
        "        context_sentence = ' '.join(instance.context)\n",
        "        # Get embedding for the context sentence\n",
        "        inputs = tokenizer(context_sentence, return_tensors='pt')\n",
        "        outputs = model(**inputs)\n",
        "        context_embedding = outputs.last_hidden_state.mean(1)\n",
        "\n",
        "        # Get synsets for the lemma\n",
        "        synsets = wn.synsets(instance.lemma)\n",
        "        max_similarity = -float('inf')\n",
        "        best_sense = None\n",
        "        for synset in synsets:\n",
        "            definition = synset.definition()\n",
        "            # Get embedding for the sense definition\n",
        "            inputs = tokenizer(definition, return_tensors='pt')\n",
        "            outputs = model(**inputs)\n",
        "            definition_embedding = outputs.last_hidden_state.mean(1)\n",
        "            # Compute cosine similarity\n",
        "            similarity = torch.cosine_similarity(context_embedding, definition_embedding)\n",
        "            if similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                best_sense = synset\n",
        "        if best_sense:\n",
        "            sense_keys = [l.key() for l in best_sense.lemmas() if l.name() == instance.lemma]\n",
        "            if sense_keys:\n",
        "                predictions[inst_id] = sense_keys[0]\n",
        "            else:\n",
        "                predictions[inst_id] = best_sense.lemmas()[0].key()\n",
        "        else:\n",
        "            predictions[inst_id] = None\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "W3oNt0xuG0Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate BERT WSD on Dev Set\n",
        "dev_bert_predictions = bert_wsd(dev_instances)\n",
        "dev_bert_accuracy = evaluate(dev_bert_predictions, dev_key)\n",
        "print('Dev BERT WSD Accuracy:', dev_bert_accuracy)\n",
        "\n",
        "# Evaluate BERT WSD on test Set\n",
        "test_bert_predictions = bert_wsd(test_instances)\n",
        "test_bert_accuracy = evaluate(test_bert_predictions, test_key)\n",
        "print('Test BERT WSD Accuracy:', test_bert_accuracy)\n"
      ],
      "metadata": {
        "id": "bnH-rLZzErPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab3f8a89-a8ad-4a17-ab41-7b7ba9708322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev BERT WSD Accuracy: 0.520618556701031\n",
            "Test BERT WSD Accuracy: 0.46620689655172415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Improved bert"
      ],
      "metadata": {
        "id": "9z9gKibg7FJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import BertTokenizerFast, BertModel\n",
        "import torch\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "def bert_wsd_improved(instances):\n",
        "    '''\n",
        "    Improved BERT WSD implementation that extracts the embedding of the target word\n",
        "    and enriches sense representations.\n",
        "    '''\n",
        "    predictions = {}\n",
        "    for inst_id, instance in instances.items():\n",
        "        context_sentence = ' '.join(instance.context)\n",
        "        target_word = instance.context[instance.index]\n",
        "        lemma = instance.lemma.lower()\n",
        "\n",
        "        # Tokenize the context sentence with offsets\n",
        "        inputs = tokenizer(context_sentence, return_tensors='pt', return_offsets_mapping=True, truncation=True)\n",
        "        offset_mapping = inputs.pop('offset_mapping')[0]\n",
        "        input_ids = inputs['input_ids'][0]\n",
        "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "        # Find the character positions of each token\n",
        "        offsets = offset_mapping.numpy()\n",
        "\n",
        "        # Find the character positions of the target word in the original sentence\n",
        "        target_word_start = sum(len(w) + 1 for w in instance.context[:instance.index])  # +1 for spaces\n",
        "        target_word_end = target_word_start + len(target_word)\n",
        "\n",
        "        # Identify token indices corresponding to the target word\n",
        "        target_token_indices = []\n",
        "        for idx, (start, end) in enumerate(offsets):\n",
        "            if start <= target_word_end and end >= target_word_start:\n",
        "                target_token_indices.append(idx)\n",
        "\n",
        "        if not target_token_indices:\n",
        "            # If target word not found, skip this instance\n",
        "            predictions[inst_id] = None\n",
        "            continue\n",
        "\n",
        "        # Get embeddings for the target word tokens\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            hidden_states = outputs.last_hidden_state.squeeze(0)\n",
        "            target_embeddings = hidden_states[target_token_indices]\n",
        "            # Average embeddings if multiple tokens\n",
        "            context_embedding = target_embeddings.mean(0).unsqueeze(0)\n",
        "\n",
        "        # Get synsets for the lemma\n",
        "        synsets = wn.synsets(lemma)\n",
        "        if not synsets:\n",
        "            predictions[inst_id] = None\n",
        "            continue\n",
        "        max_similarity = -float('inf')\n",
        "        best_sense = None\n",
        "        for synset in synsets:\n",
        "            # Create sense representation by combining definition and examples\n",
        "            sense_text = synset.definition() + ' ' + ' '.join(synset.examples())\n",
        "            # Tokenize sense text\n",
        "            inputs_def = tokenizer(sense_text, return_tensors='pt', truncation=True)\n",
        "            with torch.no_grad():\n",
        "                outputs_def = model(**inputs_def)\n",
        "                definition_embedding = outputs_def.last_hidden_state.mean(1)\n",
        "            # Compute cosine similarity\n",
        "            similarity = torch.nn.functional.cosine_similarity(context_embedding, definition_embedding)\n",
        "            if similarity.item() > max_similarity:\n",
        "                max_similarity = similarity.item()\n",
        "                best_sense = synset\n",
        "        if best_sense:\n",
        "            # Get the sense key\n",
        "            sense_keys = [l.key() for l in best_sense.lemmas() if l.name().lower() == lemma]\n",
        "            if sense_keys:\n",
        "                predictions[inst_id] = sense_keys[0]\n",
        "            else:\n",
        "                predictions[inst_id] = best_sense.lemmas()[0].key()\n",
        "        else:\n",
        "            predictions[inst_id] = None\n",
        "    return predictions\n",
        "\n",
        "# Evaluate Improved BERT WSD on Dev Set\n",
        "dev_bert_predictions = bert_wsd_improved(dev_instances)\n",
        "dev_bert_accuracy = evaluate(dev_bert_predictions, dev_key)\n",
        "print('Dev Improved BERT WSD Accuracy:', dev_bert_accuracy)\n",
        "\n",
        "# Evaluate Improved BERT WSD on Test Set\n",
        "test_bert_predictions = bert_wsd_improved(test_instances)\n",
        "test_bert_accuracy = evaluate(test_bert_predictions, test_key)\n",
        "print('Test Improved BERT WSD Accuracy:', test_bert_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0owE1R1XYg9",
        "outputId": "35bfd1f0-85e9-4503-f9bb-165963a084a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Improved BERT WSD Accuracy: 0.5773195876288659\n",
            "Test Improved BERT WSD Accuracy: 0.5524137931034483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enhanced Lesk Algorithm"
      ],
      "metadata": {
        "id": "w_UopAK34y6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enhanced_lesk(instances):\n",
        "    '''\n",
        "    Implement an enhanced Lesk algorithm that expands the sense definitions.\n",
        "    '''\n",
        "    predictions = {}\n",
        "    for inst_id, instance in instances.items():\n",
        "        # Preprocess context\n",
        "        context = preprocess(' '.join(instance.context))\n",
        "        synsets = wn.synsets(instance.lemma)\n",
        "        max_overlap = 0\n",
        "        best_sense = None\n",
        "        for synset in synsets:\n",
        "            signature = set()\n",
        "            # Include definition and examples\n",
        "            signature.update(preprocess(synset.definition()))\n",
        "            for example in synset.examples():\n",
        "                signature.update(preprocess(example))\n",
        "            # Include hypernyms and hyponyms definitions\n",
        "            for related_synset in synset.hypernyms() + synset.hyponyms():\n",
        "                signature.update(preprocess(related_synset.definition()))\n",
        "            # Compute overlap\n",
        "            overlap = len(context.intersection(signature))\n",
        "            if overlap > max_overlap:\n",
        "                max_overlap = overlap\n",
        "                best_sense = synset\n",
        "        if best_sense:\n",
        "            sense_keys = [l.key() for l in best_sense.lemmas() if l.name() == instance.lemma]\n",
        "            if sense_keys:\n",
        "                predictions[inst_id] = sense_keys[0]\n",
        "            else:\n",
        "                predictions[inst_id] = best_sense.lemmas()[0].key()\n",
        "        else:\n",
        "            predictions[inst_id] = None\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "vyaOK8884vjJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Enhanced Lesk Algorithm on Dev Set\n",
        "dev_enhanced_predictions = enhanced_lesk(dev_instances)\n",
        "dev_enhanced_accuracy = evaluate(dev_enhanced_predictions, dev_key)\n",
        "print('Dev Enhanced Lesk Accuracy:', dev_enhanced_accuracy)\n",
        "\n",
        "# Evaluate Enhanced Lesk Algorithm on Dev Set\n",
        "test_enhanced_predictions = enhanced_lesk(test_instances)\n",
        "test_enhanced_accuracy = evaluate(test_enhanced_predictions, test_key)\n",
        "print('Test Enhanced Lesk Accuracy:', test_enhanced_accuracy)\n"
      ],
      "metadata": {
        "id": "o2pIHytwFBPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561e14db-822f-4676-8044-e5a4c0addcff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Enhanced Lesk Accuracy: 0.4072164948453608\n",
            "Test Enhanced Lesk Accuracy: 0.46206896551724136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the first instance from dev_instances\n",
        "first_instance_id = next(iter(dev_instances))\n",
        "first_instance = {first_instance_id: dev_instances[first_instance_id]}\n",
        "print(first_instance.items())\n",
        "\n",
        "# Test the enhanced_lesk function\n",
        "single_enhanced_lesk_prediction = enhanced_lesk(first_instance)\n",
        "print(\"Single Instance enhanced_lesk Prediction:\", single_enhanced_lesk_prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGCKI-NLm7jF",
        "outputId": "6118b440-d601-4d40-a547-0bd959b54e05"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('d001.s001.t002', <__main__.WSDInstance object at 0x7dd58196edd0>)])\n",
            "Single Instance enhanced_lesk Prediction: {'d001.s001.t002': 'group%1:03:00::'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### yarowsky"
      ],
      "metadata": {
        "id": "tiNVxW5DZbHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def yarowsky_algorithm(dev_instances, dev_key, k=10, iterations=10):\n",
        "    '''\n",
        "    Yarowsky's algorithm for Word Sense Disambiguation.\n",
        "    - dev_instances: dictionary with the development instances\n",
        "    - dev_key: dictionary with the correct senses for the dev instances\n",
        "    - k: number of initial seeds (senses)\n",
        "    - iterations: number of iterations for spreading the seeds\n",
        "    '''\n",
        "    # 1. Initial seed set based on correct senses in the dev set\n",
        "    seeds = {}\n",
        "    for my_id, instance in dev_instances.items():\n",
        "        if my_id in dev_key:\n",
        "            correct_senses = dev_key[my_id]\n",
        "            for sense in correct_senses:\n",
        "                if sense not in seeds:\n",
        "                    seeds[sense] = set()\n",
        "                seeds[sense].add(my_id)\n",
        "\n",
        "    # 2. Iterative sense propagation based on context similarity\n",
        "    for iteration in range(iterations):\n",
        "        new_seeds = {sense: set(seeds[sense]) for sense in seeds}\n",
        "\n",
        "        for my_id, instance in dev_instances.items():\n",
        "            if my_id in dev_key:\n",
        "                continue  # Skip the labeled instances in dev set\n",
        "            context = set(instance.context)\n",
        "\n",
        "            # Find most similar sense based on context overlap\n",
        "            best_sense = None\n",
        "            best_overlap = 0\n",
        "            for sense, seed_instances in seeds.items():\n",
        "                overlap = len(context.intersection(*[set(dev_instances[id].context) for id in seed_instances]))\n",
        "                if overlap > best_overlap:\n",
        "                    best_sense = sense\n",
        "                    best_overlap = overlap\n",
        "\n",
        "            if best_sense:\n",
        "                new_seeds[best_sense].add(my_id)\n",
        "\n",
        "        seeds = new_seeds\n",
        "\n",
        "    # Assign sense to instances based on seeds\n",
        "    predicted_senses = {}\n",
        "    for my_id, instance in dev_instances.items():\n",
        "        best_sense = None\n",
        "        best_overlap = 0\n",
        "        for sense, seed_instances in seeds.items():\n",
        "            overlap = len(set(instance.context).intersection(*[set(dev_instances[id].context) for id in seed_instances]))\n",
        "            if overlap > best_overlap:\n",
        "                best_sense = sense\n",
        "                best_overlap = overlap\n",
        "        predicted_senses[my_id] = best_sense\n",
        "\n",
        "    return predicted_senses\n"
      ],
      "metadata": {
        "id": "rttdk5TTZcn6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(predicted_senses, gold_key):\n",
        "    '''\n",
        "    Calculate accuracy by comparing predicted senses to the gold key.\n",
        "    - predicted_senses: dictionary {id: sense}\n",
        "    - gold_key: dictionary {id: [list of correct senses]}\n",
        "    Returns:\n",
        "        - Accuracy as a float.\n",
        "    '''\n",
        "    correct = 0\n",
        "    total = len(predicted_senses)\n",
        "\n",
        "    for my_id, predicted_sense in predicted_senses.items():\n",
        "        if my_id in gold_key and predicted_sense in gold_key[my_id]:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / total if total > 0 else 0.0\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Apply Yarowsky's algorithm\n",
        "    predicted_dev_senses = yarowsky_algorithm(dev_instances, dev_key, k=10, iterations=10)\n",
        "    predicted_test_senses = yarowsky_algorithm(test_instances, test_key, k=10, iterations=10)\n",
        "\n",
        "    # Calculate accuracies\n",
        "    dev_accuracy = calculate_accuracy(predicted_dev_senses, dev_key)\n",
        "    test_accuracy = calculate_accuracy(predicted_test_senses, test_key)\n",
        "\n",
        "    print(f'Development Accuracy: {dev_accuracy * 100:.2f}%')\n",
        "    print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkwaVLWEZw5H",
        "outputId": "3deb3dc4-ffd9-447e-e059-881717146fc0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Development Accuracy: 14.43%\n",
            "Test Accuracy: 14.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the first instance from dev_instances\n",
        "first_instance_id = next(iter(dev_instances))\n",
        "first_instance = {first_instance_id: dev_instances[first_instance_id]}\n",
        "print(first_instance.items())\n",
        "\n",
        "first_instance_key_id = next(iter(dev_key))\n",
        "first_key = {first_instance_id: dev_key[first_instance_key_id]}\n",
        "print(first_key.items())\n",
        "\n",
        "# Test the yarowsky_algorithm function\n",
        "single_yarowsky_algorithm_prediction = yarowsky_algorithm(first_instance, first_key, k=10, iterations=10)\n",
        "print(\"Single Instance yarowsky_algorithm Prediction:\", single_yarowsky_algorithm_prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSu_PLSHnTdo",
        "outputId": "728d1135-f03f-407a-f9f3-90d1483cd42f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('d001.s001.t002', <__main__.WSDInstance object at 0x7dd58196edd0>)])\n",
            "dict_items([('d001.s001.t002', ['group%1:03:00::'])])\n",
            "Single Instance yarowsky_algorithm Prediction: {'d001.s001.t002': 'group%1:03:00::'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter tuning**"
      ],
      "metadata": {
        "id": "MGB71nK0cDhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def yarowsky_algorithm_with_params(dev_instances, dev_key, k=10, iterations=10, window_size=None):\n",
        "    '''\n",
        "    Yarowsky's algorithm with adjustable hyperparameters.\n",
        "    '''\n",
        "    # Initialize seeds\n",
        "    seeds = {}\n",
        "    for my_id, instance in dev_instances.items():\n",
        "        if my_id in dev_key:\n",
        "            correct_senses = dev_key[my_id]\n",
        "            for sense in correct_senses:\n",
        "                if sense not in seeds:\n",
        "                    seeds[sense] = set()\n",
        "                seeds[sense].add(my_id)\n",
        "\n",
        "    # Iterative sense propagation\n",
        "    for iteration in range(iterations):\n",
        "        new_seeds = {sense: set(seeds[sense]) for sense in seeds}\n",
        "        for my_id, instance in dev_instances.items():\n",
        "            if my_id in dev_key:\n",
        "                continue\n",
        "            context = set(instance.context[:window_size]) if window_size else set(instance.context)\n",
        "\n",
        "            # Find best matching sense\n",
        "            best_sense = None\n",
        "            best_overlap = 0\n",
        "            for sense, seed_instances in seeds.items():\n",
        "                overlap = len(context.intersection(*[set(dev_instances[id].context[:window_size])\n",
        "                                                     for id in seed_instances if id in dev_instances]))\n",
        "                if overlap > best_overlap:\n",
        "                    best_sense = sense\n",
        "                    best_overlap = overlap\n",
        "\n",
        "            if best_sense:\n",
        "                new_seeds[best_sense].add(my_id)\n",
        "\n",
        "        seeds = new_seeds\n",
        "\n",
        "    # Predict senses\n",
        "    predicted_senses = {}\n",
        "    for my_id, instance in dev_instances.items():\n",
        "        best_sense = None\n",
        "        best_overlap = 0\n",
        "        context = set(instance.context[:window_size]) if window_size else set(instance.context)\n",
        "        for sense, seed_instances in seeds.items():\n",
        "            overlap = len(context.intersection(*[set(dev_instances[id].context[:window_size])\n",
        "                                                 for id in seed_instances if id in dev_instances]))\n",
        "            if overlap > best_overlap:\n",
        "                best_sense = sense\n",
        "                best_overlap = overlap\n",
        "        predicted_senses[my_id] = best_sense\n",
        "\n",
        "    return predicted_senses\n",
        "\n",
        "def hyperparameter_tuning(dev_instances, dev_key, k_values, iterations_values, window_sizes):\n",
        "    '''\n",
        "    Perform hyperparameter tuning using grid search.\n",
        "    '''\n",
        "    best_accuracy = 0\n",
        "    best_params = None\n",
        "\n",
        "    for k in k_values:\n",
        "        for iterations in iterations_values:\n",
        "            for window_size in window_sizes:\n",
        "                predicted_senses = yarowsky_algorithm_with_params(dev_instances, dev_key, k=k, iterations=iterations, window_size=window_size)\n",
        "                accuracy = calculate_accuracy(predicted_senses, dev_key)\n",
        "                print(f'k={k}, iterations={iterations}, window_size={window_size}, Accuracy={accuracy * 100:.2f}%')\n",
        "\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_params = (k, iterations, window_size)\n",
        "\n",
        "    print(f'Best Parameters: k={best_params[0]}, iterations={best_params[1]}, window_size={best_params[2]}')\n",
        "    print(f'Best Development Accuracy: {best_accuracy * 100:.2f}%')\n",
        "    return best_params\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Define hyperparameter search space\n",
        "    k_values = [5, 10, 20]\n",
        "    iterations_values = [5, 10, 15]\n",
        "    window_sizes = [5, 10, None]  # None means full context\n",
        "\n",
        "    # Perform hyperparameter tuning\n",
        "    best_params = hyperparameter_tuning(dev_instances, dev_key, k_values, iterations_values, window_sizes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiKtxyI5aKGk",
        "outputId": "5265869d-a2f3-4002-eab2-a22152ed4fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k=5, iterations=5, window_size=5, Accuracy=14.43%\n",
            "k=5, iterations=5, window_size=10, Accuracy=14.43%\n",
            "k=5, iterations=5, window_size=None, Accuracy=14.43%\n",
            "k=5, iterations=10, window_size=5, Accuracy=14.43%\n",
            "k=5, iterations=10, window_size=10, Accuracy=14.43%\n",
            "k=5, iterations=10, window_size=None, Accuracy=14.43%\n",
            "k=5, iterations=15, window_size=5, Accuracy=14.43%\n",
            "k=5, iterations=15, window_size=10, Accuracy=14.43%\n",
            "k=5, iterations=15, window_size=None, Accuracy=14.43%\n",
            "k=10, iterations=5, window_size=5, Accuracy=14.43%\n",
            "k=10, iterations=5, window_size=10, Accuracy=14.43%\n",
            "k=10, iterations=5, window_size=None, Accuracy=14.43%\n",
            "k=10, iterations=10, window_size=5, Accuracy=14.43%\n",
            "k=10, iterations=10, window_size=10, Accuracy=14.43%\n",
            "k=10, iterations=10, window_size=None, Accuracy=14.43%\n",
            "k=10, iterations=15, window_size=5, Accuracy=14.43%\n",
            "k=10, iterations=15, window_size=10, Accuracy=14.43%\n",
            "k=10, iterations=15, window_size=None, Accuracy=14.43%\n",
            "k=20, iterations=5, window_size=5, Accuracy=14.43%\n",
            "k=20, iterations=5, window_size=10, Accuracy=14.43%\n",
            "k=20, iterations=5, window_size=None, Accuracy=14.43%\n",
            "k=20, iterations=10, window_size=5, Accuracy=14.43%\n",
            "k=20, iterations=10, window_size=10, Accuracy=14.43%\n",
            "k=20, iterations=10, window_size=None, Accuracy=14.43%\n",
            "k=20, iterations=15, window_size=5, Accuracy=14.43%\n",
            "k=20, iterations=15, window_size=10, Accuracy=14.43%\n",
            "k=20, iterations=15, window_size=None, Accuracy=14.43%\n",
            "Best Parameters: k=5, iterations=5, window_size=5\n",
            "Best Development Accuracy: 14.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best hyperparameters from tuning\n",
        "best_k = 5\n",
        "best_iterations = 5\n",
        "best_window_size = 5\n",
        "\n",
        "# Apply Yarowsky's algorithm to the test set using the best parameters\n",
        "predicted_test_senses = yarowsky_algorithm_with_params(\n",
        "    test_instances, test_key,\n",
        "    k=best_k,\n",
        "    iterations=best_iterations,\n",
        "    window_size=best_window_size\n",
        ")\n",
        "\n",
        "# Calculate and print test accuracy\n",
        "test_accuracy = calculate_accuracy(predicted_test_senses, test_key)\n",
        "print(f'Test Accuracy using Best Hyperparameters: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHbTPERlah5b",
        "outputId": "4e56ae1f-390a-4414-bc80-e4e936d6a65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy using Best Hyperparameters: 14.00%\n"
          ]
        }
      ]
    }
  ]
}